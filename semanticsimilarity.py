To generate word vector embeddings using `roberta-base` and then train a Gensim `Word2Vec` model from these embeddings for obtaining contextual semantic similarity between two documents, you can follow these steps. We'll break the process down into two parts: first generating the embeddings using `RoBERTa` and then training the `Word2Vec` model.

### Step 1: Generate Embeddings Using RoBERTa

We'll use the Hugging Face `transformers` library to load the `roberta-base` model and tokenizer, which can generate embeddings for text.

#### Install the necessary libraries:
```bash
pip install transformers torch gensim
```

#### Code to generate RoBERTa embeddings:
```python
import torch
from transformers import RobertaTokenizer, RobertaModel

# Load pre-trained RoBERTa model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

def get_roberta_embeddings(text):
    # Tokenize the text
    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)
    
    # Pass the inputs through the model to get the embeddings
    with torch.no_grad():
        outputs = model(**inputs)
    
    # The embeddings are in the last hidden state
    embeddings = outputs.last_hidden_state
    
    # Average the token embeddings to get a single vector for the sentence
    sentence_embedding = embeddings.mean(dim=1)
    
    return sentence_embedding.numpy()

# Example usage
doc1 = "This is the first document."
doc2 = "This is the second document."

embeddings_doc1 = get_roberta_embeddings(doc1)
embeddings_doc2 = get_roberta_embeddings(doc2)

# Now you have the embeddings for both documents
```

### Step 2: Train Gensim Word2Vec Model

Now that we have the document embeddings from RoBERTa, you can use them to train a Gensim `Word2Vec` model. However, since RoBERTa generates embeddings for the entire document or sentence, it doesn't produce word-level embeddings directly (like Word2Vec does). But you can still train a Word2Vec model using these sentence embeddings, or if you want word-level embeddings, you can break the text into smaller parts.

#### Code to Train Word2Vec on RoBERTa embeddings:
```python
from gensim.models import Word2Vec
import numpy as np

# Convert RoBERTa embeddings to a format suitable for Word2Vec
def prepare_embeddings_for_word2vec(embeddings):
    return [embedding.tolist() for embedding in embeddings]

# Prepare data for training Word2Vec (in this case, we're using sentence embeddings)
training_data = prepare_embeddings_for_word2vec([embeddings_doc1, embeddings_doc2])

# Train Word2Vec model
w2v_model = Word2Vec(sentences=training_data, vector_size=768, window=5, min_count=1, sg=0)

# Save the model
w2v_model.save("roberta_word2vec.model")

# Load the model later
# w2v_model = Word2Vec.load("roberta_word2vec.model")
```

### Step 3: Calculate Semantic Similarity

You can use the cosine similarity metric to measure the similarity between two embeddings generated by RoBERTa or Word2Vec. Here's how to do it:

#### Code to calculate similarity between two embeddings:
```python
from sklearn.metrics.pairwise import cosine_similarity

# Calculate similarity between the two document embeddings
similarity = cosine_similarity(embeddings_doc1, embeddings_doc2)

print(f"Semantic similarity between documents: {similarity[0][0]}")
```

### Summary of the Workflow:

1. **Generate RoBERTa embeddings** for your documents.
2. **Prepare these embeddings** to train a Gensim `Word2Vec` model (if desired).
3. **Train the `Word2Vec` model** on sentence-level embeddings from RoBERTa.
4. **Calculate similarity** between the document embeddings using cosine similarity.

This method allows you to combine the strengths of RoBERTa's contextual embeddings with `Word2Vec`'s flexible model training capabilities.



To accomplish the task of obtaining contextual semantic similarity between two sentences, you can follow a process involving RoBERTa-based embeddings and the Gensim Word2Vec model for soft cosine similarity. The steps can be broken down as follows:

1. **Generate Word Embeddings Using RoBERTa:**
   - Use the RoBERTa (`roberta-base`) model to generate word vector embeddings for each token in the sentences.
   - Do not average the embeddings but instead retain them for each token separately.
   
2. **Train a Gensim Word2Vec Model on Your Corpus:**
   - Train a Gensim Word2Vec model using automobile-related texts or a corpus relevant to your application to improve the accuracy of similarity scores within the domain.
   - Use this trained Word2Vec model to calculate a similarity index using soft cosine similarity.

3. **Calculate Contextual Semantic Similarity:**
   - Use the Gensim library's soft cosine similarity function, which takes into account semantic similarity between the word embeddings generated by Word2Vec and RoBERTa embeddings.
   - Soft cosine similarity allows comparison between sentences while handling out-of-vocabulary (OOV) words and spelling variations.

Here's a step-by-step guide on how you can implement this:

### Step 1: Install Necessary Libraries

You'll need the following Python libraries:
- `transformers` for generating RoBERTa embeddings.
- `torch` for handling tensors.
- `gensim` for Word2Vec and soft cosine similarity.

```bash
pip install transformers torch gensim
```

### Step 2: Generate Word Embeddings Using RoBERTa

Use the Hugging Face `transformers` library to load RoBERTa and generate token embeddings for each word.

```python
import torch
from transformers import RobertaTokenizer, RobertaModel

# Load RoBERTa model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

def get_roberta_embeddings(sentence):
    inputs = tokenizer(sentence, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**inputs)
    # Extract the last hidden state (word embeddings for each token)
    return outputs.last_hidden_state.squeeze(0)
```

### Step 3: Train Gensim Word2Vec Model

To handle the contextual similarity, train a Word2Vec model on a relevant domain-specific corpus (e.g., automobile parts text data). This step is essential to ensure that the embeddings are fine-tuned for your specific domain.

```python
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize

# Train Word2Vec model on your custom automobile parts dataset (list of tokenized sentences)
sentences = [['engine', 'oil', 'filter'], ['brake', 'pad', 'rotor'], ...]  # Add more sentences
w2v_model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)

# Save the Word2Vec model for future use
w2v_model.save("automobile_parts_word2vec.model")
```

### Step 4: Use Soft Cosine Similarity from Gensim

Gensim’s soft cosine similarity can handle the comparison between two sentences by taking into account the semantic similarity of words.

```python
from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix
from gensim import corpora
from gensim.matutils import softcossim

# Build the dictionary and corpus for Soft Cosine Similarity
sentences = [['engine', 'oil', 'filter'], ['brake', 'pad', 'rotor']]
dictionary = corpora.Dictionary(sentences)

# Create the Term Similarity Matrix using Word2Vec model
similarity_matrix = SparseTermSimilarityMatrix(w2v_model.wv, dictionary)

# Convert sentences into Bag of Words format
sentence_1 = dictionary.doc2bow(word_tokenize("oil filter replacement"))
sentence_2 = dictionary.doc2bow(word_tokenize("brake pad change"))

# Compute the soft cosine similarity between two sentences
similarity_score = softcossim(sentence_1, sentence_2, similarity_matrix)
print(f"Soft Cosine Similarity: {similarity_score}")
```

### Step 5: Combining RoBERTa Embeddings with Word2Vec Similarity (Handling OOV and Spelling Errors)

You can extend this approach by incorporating RoBERTa embeddings where Word2Vec fails (for OOV or spelling error scenarios). For each word in a sentence:
1. First, attempt to find the embedding in Word2Vec.
2. If it fails, use RoBERTa's context-aware embeddings as fallback vectors for OOV terms.

```python
def get_combined_embeddings(sentence):
    # Tokenize and try getting Word2Vec embeddings first
    tokens = word_tokenize(sentence)
    embeddings = []
    
    for token in tokens:
        if token in w2v_model.wv:
            embeddings.append(w2v_model.wv[token])
        else:
            # Fallback to RoBERTa embeddings for OOV words
            token_embedding = get_roberta_embeddings(token)
            embeddings.append(token_embedding.mean(dim=0).numpy())  # Average token embeddings
    
    return embeddings
```

Now you can compare two sentences using both RoBERTa and Word2Vec in a soft cosine similarity-based approach, which is robust enough to handle spelling errors and OOV terms.

### Summary

This approach combines RoBERTa embeddings for contextual word meanings and a Gensim Word2Vec model for domain-specific semantic similarity, using soft cosine similarity for flexible comparison between two sentences. It’s ideal for domains like automobile parts where spelling errors and OOV words may occur frequently.


To further enhance the approach with **advanced sentence alignment** and include **phrase handling** (for phrases like "fast food", "fair play", etc.) as well as **threshold-based matching**, we need to make several adjustments and improvements to the existing model. Here's an enhanced strategy and corresponding code to deal with these requirements:

### Enhancements:

1. **Advanced Sentence Alignment**:
   - We can break down long descriptions into smaller parts (e.g., sentences) and calculate similarity between each query part and each sentence in the description. We’ll focus on aligning semantically similar parts of the sentences, ensuring that phrase-based matches (like "fast food") are handled.
   - We will calculate similarity on a per-phrase level, matching the exact multi-word phrases or individual words, while keeping track of the context using RoBERTa and Word2Vec.
   
2. **Phrase Handling**:
   - Special handling for multi-word phrases like "fast food", "fair play", etc., where the meaning is dependent on the combination of words and not just individual words. 
   - We will prioritize detecting phrase embeddings first, falling back to individual word embeddings only when necessary.

3. **Threshold-based Matching**:
   - We’ll introduce a configurable threshold for similarity scores, allowing the system to determine whether a match is good enough to be considered relevant (e.g., a similarity score above 0.7).

4. **Tokenization and Matching Strategy**:
   - We will first attempt to match the query phrase with phrases in the manufacturer's description (e.g., matching "fast food" as a phrase).
   - If no phrase match is found, we will break it down into individual word tokens and match words based on their embeddings.

### Approach Outline:
- **Preprocessing**: Tokenize both the query and description, handling phrases.
- **Phrase Matching**: Prioritize phrase matching before token-level matching.
- **Advanced Sentence Alignment**: Break descriptions into sentences, and compare each part of the query with each sentence.
- **Soft Cosine Similarity**: Use soft cosine similarity on both the phrase and word level.
- **Thresholding**: Set a threshold to decide if a match is found.

### Enhanced Code:

```python
import torch
from transformers import RobertaTokenizer, RobertaModel
from gensim.models import Word2Vec
from gensim.similarities import SoftCosineSimilarity, SparseTermSimilarityMatrix
from gensim import corpora
from gensim.matutils import softcossim
from nltk.tokenize import word_tokenize, sent_tokenize

# Load RoBERTa model and tokenizer
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
model = RobertaModel.from_pretrained('roberta-base')

# Load pre-trained Word2Vec model for automobile parts (domain-specific)
w2v_model = Word2Vec.load("automobile_parts_word2vec.model")

# Define a function to get embeddings using RoBERTa for OOV words
def get_roberta_embeddings(sentence):
    inputs = tokenizer(sentence, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**inputs)
    # Extract the last hidden state (word embeddings for each token)
    return outputs.last_hidden_state.squeeze(0)

# Function to get combined embeddings from Word2Vec and fallback to RoBERTa
def get_combined_embeddings(sentence):
    tokens = word_tokenize(sentence)
    embeddings = []
    
    for token in tokens:
        if token in w2v_model.wv:  # If token exists in Word2Vec
            embeddings.append(w2v_model.wv[token])
        else:
            # Fallback to RoBERTa embeddings for OOV words
            token_embedding = get_roberta_embeddings(token)
            embeddings.append(token_embedding.mean(dim=0).numpy())  # Average token embeddings
    
    return embeddings

# Step 1: Preprocess sentence by checking for multi-word phrases first
def preprocess_sentence(sentence, phrase_list):
    tokenized_sentence = word_tokenize(sentence)
    processed_tokens = []
    
    i = 0
    while i < len(tokenized_sentence):
        found_phrase = False
        # Check for phrases
        for phrase in phrase_list:
            phrase_tokens = word_tokenize(phrase)
            # If phrase matches, add it as a single token
            if tokenized_sentence[i:i+len(phrase_tokens)] == phrase_tokens:
                processed_tokens.append('_'.join(phrase_tokens))  # Join phrase with underscores
                i += len(phrase_tokens)  # Move index by phrase length
                found_phrase = True
                break
        if not found_phrase:
            processed_tokens.append(tokenized_sentence[i])
            i += 1
    return processed_tokens

# Example of common phrases
phrase_list = ["fast food", "fair play", "oil filter", "brake pad"]

# Preprocess both query and description to detect phrases
def get_processed_embeddings(sentence, phrase_list):
    processed_sentence = preprocess_sentence(sentence, phrase_list)
    processed_sentence_text = ' '.join(processed_sentence).replace('_', ' ')
    return get_combined_embeddings(processed_sentence_text)

# Generate embeddings for both query and manufacturer's description
def get_sentence_embeddings(sentence, phrase_list):
    return get_processed_embeddings(sentence, phrase_list)

# Advanced sentence alignment by splitting descriptions into sentences
def compute_similarity(query, description, phrase_list, threshold=0.7):
    query_embeddings = get_sentence_embeddings(query, phrase_list)
    
    # Split the manufacturer description into sentences
    description_sentences = sent_tokenize(description)
    
    # Store similarity scores for each sentence
    similarity_scores = []
    
    # Create dictionary for all tokenized phrases
    all_sentences = [preprocess_sentence(query, phrase_list)] + [preprocess_sentence(sent, phrase_list) for sent in description_sentences]
    dictionary = corpora.Dictionary(all_sentences)
    
    # Create the term similarity matrix using Word2Vec
    similarity_matrix = SparseTermSimilarityMatrix(w2v_model.wv, dictionary)
    
    # Convert query into Bag of Words format
    query_bow = dictionary.doc2bow(preprocess_sentence(query, phrase_list))
    
    for sentence in description_sentences:
        sentence_bow = dictionary.doc2bow(preprocess_sentence(sentence, phrase_list))
        
        # Calculate soft cosine similarity for each sentence
        similarity_score = softcossim(query_bow, sentence_bow, similarity_matrix)
        similarity_scores.append(similarity_score)
    
    # Get the highest similarity score across sentences
    max_similarity_score = max(similarity_scores)
    
    print(f"Max Soft Cosine Similarity: {max_similarity_score}")
    
    # Determine if match is found based on threshold
    if max_similarity_score > threshold:
        print("Spare part matched in the manufacturer's description.")
        return True, max_similarity_score
    else:
        print("No significant match found.")
        return False, max_similarity_score

# Test example
spare_part_query = "oil filter replacement"  # Query (part to search)
manufacturer_description = "This product includes an oil filter and engine parts for your vehicle."  # Description (manufacturer)

# Run the similarity comparison
is_match, similarity_score = compute_similarity(spare_part_query, manufacturer_description, phrase_list, threshold=0.7)
```

### Explanation of Enhancements:

#### 1. **Phrase Handling**:
   - We first preprocess both the query and the description to **detect phrases** (e.g., "fast food", "oil filter"). These are treated as single units in tokenization (replacing spaces with underscores like `fast_food`).
   - If a phrase match is found, it is considered a single token, ensuring that multi-word phrases are given appropriate contextual weight.

#### 2. **Advanced Sentence Alignment**:
   - Descriptions are split into individual **sentences** using the `sent_tokenize` function.
   - The query is compared against each sentence in the description. This helps in finding partial matches in longer documents, where the relevant part may appear in only one sentence.
   - For each sentence, **soft cosine similarity** is computed. The highest similarity score across all sentences is retained.

#### 3. **Soft Cosine Similarity**:
   - We calculate **soft cosine similarity** between the query and each sentence from the description, considering both individual words and multi-word phrases.
   - We use the **SparseTermSimilarityMatrix** created from Word2Vec to compute the similarity between words, with RoBERTa embeddings as a fallback for OOV terms.

#### 4. **Threshold-based Matching**:
   - A **threshold** (configurable, e.g., 0.7) is introduced to determine whether the similarity score is high enough to consider the part "matched" in the manufacturer's description.
   - If the maximum similarity score across sentences exceeds this threshold, the system will report a match.

### Phrase Handling (e.g., "Fast Food", "Fair Play"):
- The preprocessing function handles **phrases** by detecting multi-word terms in both the query and description.
- By joining phrases with underscores, it treats them as single tokens, allowing the system to preserve their meaning as combined units rather than individual words.

### Threshold for Matching:
- A **similarity threshold** (default: 0.7) determines whether a match is good enough. This allows fine-tuning based on domain-specific needs. If the score is above the threshold, we can confidently say that the spare part is present in the description.

### Conclusion:
This enhanced approach uses **advanced sentence alignment**, **phrase handling**, and **threshold-based matching** to provide a robust solution for checking if an automobile spare part is present in a manufacturer's description. It effectively handles multi-word phrases (like "fast food", "oil filter") and provides contextual, semantic similarity using both RoBERTa and Word2Vec.



Let's enhance the previous code by adding a function to automatically generate the **most frequently occurring multi-word phrases** and a function to **train a Word2Vec model** for the automobile spare parts domain. We'll also provide code for training the Word2Vec model and links to download pretrained models and domain-specific datasets.

### 1. **Function to Extract Most Frequent Multi-Word Phrases**:
To extract frequent multi-word phrases, we’ll use the **Gensim Phrases model**. This model can detect common bigrams (two-word phrases), trigrams (three-word phrases), etc., from a corpus.

### 2. **Train Word2Vec Model on Automobile Parts Data**:
We’ll create a function to train a **Word2Vec model** on domain-specific data related to automobile parts. This will involve tokenizing the data and then training the model using Gensim’s `Word2Vec` API.

### 3. **Reference Links** for Pre-trained Models and Automobile Spare Parts Dataset:
We'll provide some useful references to pre-trained Word2Vec models and public automobile datasets you can use for training or testing.

---

### Function to Extract Most Frequent Multi-Word Phrases

Here’s a function to extract **most frequent multi-word phrases** (bigrams, trigrams) from a corpus using Gensim's `Phrases` model:

```python
from gensim.models.phrases import Phrases, Phraser
from nltk.tokenize import word_tokenize
from collections import Counter
import re

# Function to clean text
def clean_text(text):
    text = re.sub(r'\W', ' ', text)  # Remove all non-word characters
    text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with a single space
    return text.strip().lower()

# Function to extract most frequent multi-word phrases (bigrams/trigrams)
def extract_frequent_phrases(corpus, min_count=5, threshold=10.0):
    """
    Extracts the most frequent bigrams and trigrams from a corpus.
    :param corpus: List of documents (each document is a string)
    :param min_count: Minimum frequency for a phrase to be considered
    :param threshold: Score threshold for forming phrases
    :return: List of most frequent phrases
    """
    # Tokenize the corpus into words
    tokenized_corpus = [word_tokenize(clean_text(doc)) for doc in corpus]
    
    # Create a bigram model
    bigram_model = Phrases(tokenized_corpus, min_count=min_count, threshold=threshold)
    bigram_phraser = Phraser(bigram_model)
    
    # Apply bigram model to the corpus to get phrases
    bigram_corpus = list(bigram_phraser[tokenized_corpus])
    
    # Create a trigram model based on the bigram corpus
    trigram_model = Phrases(bigram_corpus, min_count=min_count, threshold=threshold)
    trigram_phraser = Phraser(trigram_model)
    
    # Apply trigram model to get phrases
    trigram_corpus = list(trigram_phraser[bigram_corpus])
    
    # Flatten the trigram corpus and count phrase frequencies
    phrases = ['_'.join(words) for sentence in trigram_corpus for words in sentence if '_' in words]
    phrase_counts = Counter(phrases)
    
    # Return the most frequent phrases
    return phrase_counts.most_common()

# Sample test corpus
sample_corpus = [
    "The oil filter for this car is located near the engine.",
    "This spare part includes the brake pad and the engine oil filter.",
    "An engine oil filter is essential for vehicle maintenance."
]

# Extract and display most frequent phrases
frequent_phrases = extract_frequent_phrases(sample_corpus)
print(frequent_phrases)
```

### Explanation:
- **Gensim's Phrases model** automatically detects common word combinations and joins them using an underscore (`_`), turning "oil filter" into `"oil_filter"`.
- You can adjust `min_count` to control how frequently a phrase must appear to be considered, and `threshold` to adjust the sensitivity for phrase formation.
- The function outputs a list of the most frequent multi-word phrases (e.g., "oil_filter", "brake_pad").

---

### Code to Train Word2Vec Model for Automobile Parts

Now, let's create a function to **train a Word2Vec model** for automobile parts using a domain-specific dataset.

```python
from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import re

# Function to clean and tokenize text (suitable for automobile domain)
def preprocess_text(text):
    text = re.sub(r'\W', ' ', text)  # Remove non-alphanumeric characters
    text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces with a single space
    return word_tokenize(text.lower().strip())

# Function to train Word2Vec model
def train_word2vec_model(corpus, vector_size=100, window=5, min_count=5, workers=4, sg=1):
    """
    Train a Word2Vec model on the given corpus.
    :param corpus: List of documents (each document is a string)
    :param vector_size: Dimensionality of the word vectors
    :param window: Maximum distance between the current and predicted word within a sentence
    :param min_count: Ignores all words with total frequency lower than this
    :param workers: Number of worker threads to use
    :param sg: Skip-gram (1) or CBOW (0) model
    :return: Trained Word2Vec model
    """
    # Preprocess the corpus
    tokenized_corpus = [preprocess_text(doc) for doc in corpus]
    
    # Train the Word2Vec model
    model = Word2Vec(sentences=tokenized_corpus, vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=sg)
    
    return model

# Sample automobile parts-related corpus
sample_corpus = [
    "oil filter replacement for car engine",
    "brake pad for vehicle brake system",
    "spark plug for engine combustion chamber",
    "car tire replacement and maintenance service",
    "air filter for car engine air intake"
]

# Train Word2Vec model
w2v_model = train_word2vec_model(sample_corpus)

# Save the trained model for future use
w2v_model.save("automobile_parts_word2vec.model")

# Test: Get similar words
print(w2v_model.wv.most_similar("oil_filter"))
```

### Explanation:
- The `train_word2vec_model` function takes a **corpus** of documents related to automobile parts and trains a Word2Vec model.
- The Word2Vec model can be configured using parameters like `vector_size` (dimension of the word vectors), `window` (context window size), `min_count` (minimum frequency for a word to be considered), and `sg` (1 for skip-gram, 0 for CBOW).
- After training, you can use the `wv.most_similar()` method to retrieve semantically similar words in the automobile domain (e.g., finding words related to "oil_filter").

---

### References for Pre-Trained Models and Datasets

#### Pre-trained Word2Vec Models:
1. **Google's Pre-trained Word2Vec**:
   - Google offers a pre-trained Word2Vec model trained on **Google News** with 300-dimensional vectors.
   - Download link: [Google Word2Vec Pre-trained Model](https://code.google.com/archive/p/word2vec/)
   
2. **GloVe (Global Vectors for Word Representation)**:
   - An alternative to Word2Vec, pre-trained on various datasets like Wikipedia and Common Crawl.
   - Download link: [GloVe Pre-trained Models](https://nlp.stanford.edu/projects/glove/)

3. **FastText (Pre-trained Subword Embeddings)**:
   - Facebook AI Research provides FastText, which is particularly good at handling out-of-vocabulary (OOV) words by using subword information.
   - Download link: [FastText Pre-trained Models](https://fasttext.cc/docs/en/crawl-vectors.html)

#### Automobile Spare Parts Datasets:
1. **Kaggle Datasets**:
   - Kaggle has several datasets related to automobile parts and maintenance.
   - Example: [Automobile Parts Dataset on Kaggle](https://www.kaggle.com/datasets/shivamb/automobile-dataset)
   
2. **UCI Machine Learning Repository**:
   - The UCI repository offers datasets like **Car Evaluation** and **Automobile** datasets that could be useful for training models related to vehicles.
   - Link: [UCI Car Evaluation Dataset](https://archive.ics.uci.edu/ml/datasets/Car+Evaluation)

3. **PartSouq**:
   - An online marketplace for automobile parts with large amounts of descriptive data.
   - You can scrape the website for descriptions of various parts.
   - Link: [PartSouq - Online Auto Parts](https://www.partsouq.com/)

---

### Conclusion:
- **Multi-word phrases** are extracted using Gensim's Phrases model, making it easier to handle compound terms like "fast food" and "brake pad".
- The **Word2Vec training function** lets you train a custom model on domain-specific automobile data, ensuring better performance in identifying relationships between parts.
- **Pre-trained models** like Google Word2Vec, GloVe, and FastText provide strong baselines, while **datasets from Kaggle or UCI** allow you
